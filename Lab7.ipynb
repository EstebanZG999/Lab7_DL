{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f3b3198",
   "metadata": {},
   "source": [
    "# Laboratorio 7 Deep Learning\n",
    "\n",
    "Laboratorio 7 – Deep Learning\n",
    "\n",
    "Edwin Ortega 22305 - Esteban Zambrano 22119\n",
    "\n",
    "Link del repositorio:<br>\n",
    "https://github.com/EstebanZG999/Lab7_DL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce2270d",
   "metadata": {},
   "source": [
    "### Task 1 - Práctica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbdaca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gymnasium numpy torch matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121ebbc2",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de20c5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31389a75",
   "metadata": {},
   "source": [
    "#### Configuración y utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4923593",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ENV_ID = \"CartPole-v1\"\n",
    "env = gym.make(ENV_ID)\n",
    "eval_env = gym.make(ENV_ID)\n",
    "\n",
    "obs, _ = env.reset(seed=SEED)\n",
    "\n",
    "n_states = env.observation_space.shape[0]    # 4\n",
    "n_actions = env.action_space.n               # 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9821695",
   "metadata": {},
   "source": [
    "#### Red Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4be0ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int, hidden: int = 128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6269b7e",
   "metadata": {},
   "source": [
    "#### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34b3b971",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = collections.namedtuple(\n",
    "    \"Transition\", (\"state\", \"action\", \"reward\", \"next_state\", \"done\")\n",
    ")\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        return Transition(*zip(*batch))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ef8e25",
   "metadata": {},
   "source": [
    "#### Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "630c0783",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_CAPACITY = 50_000\n",
    "\n",
    "# Exploración (epsilon-greedy)\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 10_000 \n",
    "\n",
    "# Sincronización de red objetivo\n",
    "TARGET_SYNC_EVERY = 1000  # pasos\n",
    "USE_SOFT_UPDATE = False\n",
    "TAU = 0.005\n",
    "\n",
    "# Entrenamiento\n",
    "MAX_EPISODES = 400\n",
    "MAX_STEPS_PER_EP = 1000\n",
    "RENDER_DURING_TRAIN = False  # True para visualizar\n",
    "\n",
    "# Evaluación\n",
    "EVAL_EPISODES = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2049c635",
   "metadata": {},
   "source": [
    "#### Inicialización de redes y optimizador\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70cf2ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = QNetwork(n_states, n_actions).to(device)\n",
    "target_net = QNetwork(n_states, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "replay = ReplayBuffer(BUFFER_CAPACITY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551226f3",
   "metadata": {},
   "source": [
    "#### Selección de acción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "921695cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "\n",
    "def epsilon_by_step(step: int) -> float:\n",
    "    # Exponencial suave: eps = EPS_END + (EPS_START - EPS_END) * exp(-step/decay)\n",
    "    return EPS_END + (EPS_START - EPS_END) * math.exp(-step / EPS_DECAY)\n",
    "\n",
    "def select_action(state: np.ndarray) -> int:\n",
    "    global steps_done\n",
    "    eps = epsilon_by_step(steps_done)\n",
    "    steps_done += 1\n",
    "    if random.random() < eps:\n",
    "        return env.action_space.sample()\n",
    "    with torch.no_grad():\n",
    "        s = torch.as_tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        q = policy_net(s)\n",
    "        return int(q.argmax(dim=1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519aade6",
   "metadata": {},
   "source": [
    "#### Actualización de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "083a8155",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = nn.SmoothL1Loss()  # Huber loss\n",
    "\n",
    "def optimize_model():\n",
    "    if len(replay) < BATCH_SIZE:\n",
    "        return None\n",
    "\n",
    "    batch = replay.sample(BATCH_SIZE)\n",
    "\n",
    "    state_batch = torch.as_tensor(np.array(batch.state), dtype=torch.float32, device=device)\n",
    "    action_batch = torch.as_tensor(batch.action, dtype=torch.int64, device=device).unsqueeze(1)\n",
    "    reward_batch = torch.as_tensor(batch.reward, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "    next_state_batch = torch.as_tensor(np.array(batch.next_state), dtype=torch.float32, device=device)\n",
    "    done_batch = torch.as_tensor(batch.done, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "    # Q(s,a) actual de la policy_net\n",
    "    q_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Q objetivo: r + gamma * max_a' Q_target(s', a') * (1 - done)\n",
    "    with torch.no_grad():\n",
    "        next_q = target_net(next_state_batch).max(dim=1, keepdim=True)[0]\n",
    "        target = reward_batch + (1.0 - done_batch) * GAMMA * next_q\n",
    "\n",
    "    loss = mse_loss(q_values, target)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 10.0)\n",
    "    optimizer.step()\n",
    "    return float(loss.item())\n",
    "\n",
    "def hard_update_target():\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "def soft_update_target(tau: float):\n",
    "    with torch.no_grad():\n",
    "        for p, tp in zip(policy_net.parameters(), target_net.parameters()):\n",
    "            tp.data.mul_(1 - tau).add_(tau * p.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff8e8f3",
   "metadata": {},
   "source": [
    "#### Ciclo de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127463ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episodio 010] Recompensa media (últimos 10): 24.9 | epsilon≈0.977\n",
      "[Episodio 020] Recompensa media (últimos 10): 17.9 | epsilon≈0.960\n",
      "[Episodio 030] Recompensa media (últimos 10): 22.7 | epsilon≈0.940\n",
      "[Episodio 040] Recompensa media (últimos 10): 26.9 | epsilon≈0.916\n",
      "[Episodio 050] Recompensa media (últimos 10): 23.5 | epsilon≈0.896\n",
      "[Episodio 060] Recompensa media (últimos 10): 24.8 | epsilon≈0.875\n",
      "[Episodio 070] Recompensa media (últimos 10): 23.4 | epsilon≈0.856\n",
      "[Episodio 080] Recompensa media (últimos 10): 26.2 | epsilon≈0.835\n",
      "[Episodio 090] Recompensa media (últimos 10): 17.8 | epsilon≈0.822\n",
      "[Episodio 100] Recompensa media (últimos 10): 27.2 | epsilon≈0.801\n",
      "[Episodio 110] Recompensa media (últimos 10): 35.7 | epsilon≈0.774\n",
      "[Episodio 120] Recompensa media (últimos 10): 26.4 | epsilon≈0.756\n",
      "[Episodio 130] Recompensa media (últimos 10): 32.0 | epsilon≈0.733\n",
      "[Episodio 140] Recompensa media (últimos 10): 26.2 | epsilon≈0.716\n",
      "[Episodio 150] Recompensa media (últimos 10): 22.9 | epsilon≈0.701\n",
      "[Episodio 160] Recompensa media (últimos 10): 19.8 | epsilon≈0.688\n",
      "[Episodio 170] Recompensa media (últimos 10): 19.6 | epsilon≈0.676\n",
      "[Episodio 180] Recompensa media (últimos 10): 38.3 | epsilon≈0.652\n",
      "[Episodio 190] Recompensa media (últimos 10): 32.7 | epsilon≈0.633\n",
      "[Episodio 200] Recompensa media (últimos 10): 35.9 | epsilon≈0.612\n",
      "[Episodio 210] Recompensa media (últimos 10): 34.4 | epsilon≈0.593\n",
      "[Episodio 220] Recompensa media (últimos 10): 31.9 | epsilon≈0.576\n",
      "[Episodio 230] Recompensa media (últimos 10): 39.9 | epsilon≈0.555\n",
      "[Episodio 240] Recompensa media (últimos 10): 48.5 | epsilon≈0.532\n",
      "[Episodio 250] Recompensa media (últimos 10): 45.1 | epsilon≈0.510\n",
      "[Episodio 260] Recompensa media (últimos 10): 17.9 | epsilon≈0.502\n",
      "[Episodio 270] Recompensa media (últimos 10): 38.2 | epsilon≈0.485\n",
      "[Episodio 280] Recompensa media (últimos 10): 49.6 | epsilon≈0.464\n",
      "[Episodio 290] Recompensa media (últimos 10): 48.9 | epsilon≈0.444\n",
      "[Episodio 300] Recompensa media (últimos 10): 76.7 | epsilon≈0.415\n",
      "[Episodio 310] Recompensa media (últimos 10): 37.0 | epsilon≈0.402\n",
      "[Episodio 320] Recompensa media (últimos 10): 52.5 | epsilon≈0.384\n",
      "[Episodio 330] Recompensa media (últimos 10): 33.2 | epsilon≈0.373\n",
      "[Episodio 340] Recompensa media (últimos 10): 59.9 | epsilon≈0.354\n",
      "[Episodio 350] Recompensa media (últimos 10): 43.4 | epsilon≈0.341\n",
      "[Episodio 360] Recompensa media (últimos 10): 40.6 | epsilon≈0.330\n",
      "[Episodio 370] Recompensa media (últimos 10): 82.4 | epsilon≈0.308\n",
      "[Episodio 380] Recompensa media (últimos 10): 46.7 | epsilon≈0.296\n",
      "[Episodio 390] Recompensa media (últimos 10): 28.3 | epsilon≈0.289\n",
      "[Episodio 400] Recompensa media (últimos 10): 78.8 | epsilon≈0.271\n"
     ]
    }
   ],
   "source": [
    "episode_rewards = []\n",
    "episode_losses = []\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "for ep in range(1, MAX_EPISODES + 1):\n",
    "    state, _ = env.reset(seed=SEED + ep)\n",
    "    ep_reward = 0.0\n",
    "    ep_losses = []\n",
    "\n",
    "    for t in range(MAX_STEPS_PER_EP):\n",
    "        if RENDER_DURING_TRAIN:\n",
    "            env.render()\n",
    "\n",
    "        action = select_action(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # shaping\n",
    "        replay.push(state, action, reward, next_state, float(done))\n",
    "\n",
    "        state = next_state\n",
    "        ep_reward += reward\n",
    "        global_step += 1\n",
    "\n",
    "        loss = optimize_model()\n",
    "        if loss is not None:\n",
    "            ep_losses.append(loss)\n",
    "\n",
    "        # Actualizar red objetivo\n",
    "        if USE_SOFT_UPDATE:\n",
    "            soft_update_target(TAU)\n",
    "        else:\n",
    "            if global_step % TARGET_SYNC_EVERY == 0:\n",
    "                hard_update_target()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    episode_rewards.append(ep_reward)\n",
    "    episode_losses.append(np.mean(ep_losses) if ep_losses else np.nan)\n",
    "\n",
    "    if ep % 10 == 0:\n",
    "        avg_last = np.mean(episode_rewards[-10:])\n",
    "        print(f\"[Episodio {ep:03d}] Recompensa media (últimos 10): {avg_last:.1f} | \"\n",
    "              f\"epsilon≈{epsilon_by_step(global_step):.3f}\")\n",
    "\n",
    "# Cerrar render si se usó\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
